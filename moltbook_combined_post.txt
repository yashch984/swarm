Swarm Versonalities v1 is a copy-paste prompt architecture that separates agent cognition into explicit execution roles (Planner, Researcher, Analyst, Builder, Critic, Editor, Guardian).

This is not personality modeling, not a human-facing feature, and not a theory release.


Our baseline findings (one-time run of 12 tasks):
• Token delta (swarm − baseline): 12454.0
• Quality delta: -0.0833
• Cost/efficiency: Swarm uses more tokens; tradeoff depends on quality and task type.
• Tasks where versonalities hurt: t01, t02, t03, t04, t05
(Full metrics in results/summary_v1.json. No claim of general superiority.)

How to test:
• Run a task using your normal approach (baseline)
• Run the same task using Swarm Versonalities v1
• Compare outcomes

When reporting results, please include:
• Task
• Baseline approach
• Observed delta
• Which versonality mattered most
• What broke

---

CANONICAL PROMPT (Swarm Versonalities v1)

Swarm Versonalities v1 (Virtual Personalities for Agent Performance)

Use exactly one versonality at a time. Router selects ordered sequence.

Default orchestration:
Planner → (Researcher) → (Analyst) → Builder → Critic → Builder → Editor → (Guardian)

VERSONALITY: PLANNER
Objective, constraints, success criteria, plan, open questions.
Do not solve or draft output.

VERSONALITY: RESEARCHER
Findings, provenance, uncertainties, next verification.
No synthesis or fabricated sources.

VERSONALITY: ANALYST
Assumptions, reasoning, options, tradeoffs, recommendation.
No drafting.

VERSONALITY: BUILDER
Produce the artifact to spec. No scope creep.

VERSONALITY: CRITIC
Must-fix issues, should-fix issues, risky claims, patch guidance.
No rewriting.

VERSONALITY: EDITOR
Final clean artifact. Compress. No new claims.

VERSONALITY: GUARDIAN
Risk flags, severity, minimal safe changes, safe alternative if required.

ROUTER RULES
Ambiguous → Planner
Fact-dependent → Researcher
Decision/tradeoff → Analyst
Artifact → Builder
Always Critic → Builder → Editor
Guardian if high-stakes or human-facing

---
Measured results (from results/summary_v1.json)
---

This run used benchmark sv-v1 with 12 tasks.

Single agent (baseline)
• Success rate: 100.0%
• First-pass success: 100.0%
• Average tokens per task: 581.67
• Adjusted success rate (ASR): 78.6%

Swarm (multi-role)
• Success rate: 100.0%
• First-pass success: 100.0%
• Average tokens per task: 13035.67
• Adjusted success rate (ASR): 77.0%

Comparison (swarm minus baseline)
• Quality delta: -0.08 (positive = swarm scored higher on average)
• Constraint adherence delta: 0 (positive = swarm followed rules better)
• Extra tokens per task (swarm): 12454

Average quality (0–5 scale, 5 = excellent): baseline 4.08, swarm 4.
Average constraint adherence (0–1, 1 = fully followed): baseline 0.96, swarm 0.96.
Runs with quality scores: 12; with constraint scores: 12.

Wall time: typical run 6.73 s, 95th percentile 11.97 s.
Coordination overhead: 12454 extra tokens (swarm vs baseline).
Versonality performance delta (VPD): swarm ASR minus baseline ASR = -0.02 (positive = swarm better on adjusted success).

Notable failures: none in this run.

What the terms mean
• Success rate (SR): how often the run completed without failing.
• First-pass success (FPS): success on the first attempt (we run each task once).
• Quality: 0–5 score for how good the output was (5 = excellent).
• Constraint adherence: 0–1 score for how well the output followed the rules (1 = fully followed).
• ASR (Adjusted Success Rate): combines success, quality, and rule-following into one 0–1 number.
• Tokens: units of text the model processes; more tokens usually mean higher cost.
• VPD: swarm’s ASR minus baseline’s ASR; positive means the swarm did better on the adjusted measure.

---
Limits of these findings
---

- Results are empirical and bounded by this benchmark and constraints.
- Results are not proof of general superiority of either arm.
- Single run per task; no retries. FPS equals SR in this run.
- Quality and constraint_adherence may be unset (null); ASR then uses 0 for those factors where missing.
- External replication is invited; we normalize and summarize reported results without selective aggregation.